{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(file_path):\n",
    "    \"\"\"\n",
    "    Read and preprocess the text from a file.\n",
    "    \n",
    "    Args:\n",
    "    file_path (str): Path to the text file\n",
    "    \n",
    "    Returns:\n",
    "    list: List of preprocessed tokens\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower()\n",
    "    \n",
    "    # Tokenize without removing stopwords\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Only remove non-alphanumeric tokens\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def create_term_term_matrix(tokens, window_size=5):\n",
    "    \"\"\"\n",
    "    Create a term-term matrix based on co-occurrence within a specified window.\n",
    "    \n",
    "    Args:\n",
    "    tokens (list): List of preprocessed tokens\n",
    "    window_size (int): Size of the co-occurrence window\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (term-term matrix, vocabulary)\n",
    "    \"\"\"\n",
    "    vocab = sorted(set(tokens))\n",
    "    word_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    matrix = np.zeros((len(vocab), len(vocab)), dtype=int)\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(max(0, i-window_size), min(len(tokens), i+window_size+1)):\n",
    "            if i != j:\n",
    "                word1, word2 = tokens[i], tokens[j]\n",
    "                matrix[word_to_id[word1], word_to_id[word2]] += 1\n",
    "    \n",
    "    return matrix, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_heatmap(matrix, vocab, title, top_n=50):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of the term-term matrix for the top N most frequent terms.\n",
    "    \n",
    "    Args:\n",
    "    matrix (numpy.ndarray): Term-term matrix\n",
    "    vocab (list): Vocabulary list\n",
    "    title (str): Title for the heatmap\n",
    "    top_n (int): Number of top terms to include in the heatmap\n",
    "    \"\"\"\n",
    "    # Get the top N most frequent terms\n",
    "    term_frequencies = matrix.sum(axis=1)\n",
    "    top_indices = term_frequencies.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    top_matrix = matrix[top_indices][:, top_indices]\n",
    "    top_vocab = [vocab[i] for i in top_indices]\n",
    "    \n",
    "    plt.figure(figsize=(15, 13))\n",
    "    sns.heatmap(top_matrix, xticklabels=top_vocab, yticklabels=top_vocab, cmap='YlOrRd')\n",
    "    plt.title(f'{title} (Top {top_n} Terms)')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "br_constitution_path = 'path/to/brazilian_constitution.txt'\n",
    "us_constitution_path = 'path/to/us_constitution.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Brazilian Constitution\n",
    "br_tokens = preprocess_text(br_constitution_path)\n",
    "br_matrix, br_vocab = create_term_term_matrix(br_tokens)\n",
    "plot_heatmap(br_matrix, br_vocab, 'Brazilian Constitution Term-Term Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process US Constitution\n",
    "us_tokens = preprocess_text(us_constitution_path)\n",
    "us_matrix, us_vocab = create_term_term_matrix(us_tokens)\n",
    "plot_heatmap(us_matrix, us_vocab, 'US Constitution Term-Term Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some statistics\n",
    "print(f\"Brazilian Constitution: {len(br_tokens)} tokens, {len(br_vocab)} unique terms\")\n",
    "print(f\"US Constitution: {len(us_tokens)} tokens, {len(us_vocab)} unique terms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find top co-occurring terms for a specific word in Brazilian Constitution\n",
    "word = 'de'  # Portuguese for 'of'\n",
    "if word in br_vocab:\n",
    "    word_id = br_vocab.index(word)\n",
    "    co_occurrences = br_matrix[word_id]\n",
    "    top_10 = sorted(zip(br_vocab, co_occurrences), key=lambda x: x[1], reverse=True)[:11]\n",
    "    print(f\"\\nTop 10 co-occurring terms with '{word}' in Brazilian Constitution:\")\n",
    "    for term, count in top_10[1:]:  # Skip the first one as it's the word itself\n",
    "        print(f\"{term}: {count}\")\n",
    "else:\n",
    "    print(f\"'{word}' not found in Brazilian Constitution vocabulary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for US Constitution with an English word\n",
    "word = 'of'\n",
    "if word in us_vocab:\n",
    "    word_id = us_vocab.index(word)\n",
    "    co_occurrences = us_matrix[word_id]\n",
    "    top_10 = sorted(zip(us_vocab, co_occurrences), key=lambda x: x[1], reverse=True)[:11]\n",
    "    print(f\"\\nTop 10 co-occurring terms with '{word}' in US Constitution:\")\n",
    "    for term, count in top_10[1:]:  # Skip the first one as it's the word itself\n",
    "        print(f\"{term}: {count}\")\n",
    "else:\n",
    "    print(f\"'{word}' not found in US Constitution vocabulary\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
