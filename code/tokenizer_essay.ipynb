{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import regex as re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"Petitioner John Angus Smith and his companion went from Tennessee to Florida to buy cocaine; they hoped to resell it at a profit. While in Florida, they met petitioner's acquaintance, Deborah Hoag. Hoag agreed to, and in fact did, purchase cocaine for petitioner.\\\n",
    "    She then accompanied petitioner and his friend to her motel room, where they were joined by a drug dealer. While Hoag listened, petitioner and the dealer discussed petitioner's MAC–10 firearm, which had been modified to operate as an automatic. The MAC–10 apparently is a \\\n",
    "    favorite among criminals. It is small and compact, lightweight, and can be equipped with a silencer. Most important of all, it can be devastating: A fully automatic MAC–10 can fire more than 1,000 rounds per minute. The dealer expressed his interest in becoming the owner \\\n",
    "    of a MAC–10, and petitioner promised that he would discuss selling the gun if his arrangement with another potential buyer fell through. Unfortunately for petitioner, Hoag had contacts not only with narcotics traffickers but also with law enforcement officials. \\\n",
    "    In fact, she was a confidential informant. Consistent with her post, she informed the Broward County Sheriff's Office of petitioner's activities. The Sheriff's Office responded quickly, sending an undercover officer to Hoag's motel room. Several others were assigned to \\\n",
    "    keep the motel under surveillance. Upon arriving at Hoag's motel room, the undercover officer presented himself to petitioner as a pawnshop dealer. Petitioner, in turn, presented the officer with a proposition: He had an automatic MAC–10 and silencer with which he might \\\n",
    "    be willing to part. Petitioner then pulled the MAC–10 out of a black canvas bag and showed it to the officer. The officer examined the gun and asked petitioner what he wanted for it. Rather than asking for money, however, petitioner asked for drugs. He was willing to trade his MAC–10, he said, \\\n",
    "    for two ounces of cocaine. The officer told petitioner that he was just a pawnshop dealer and did not distribute narcotics. Nonetheless, he indicated that he wanted the MAC–10 and would try to get the cocaine. The officer then left, promising to return within an hour.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a byte tokenizer\n",
    "def byte_tokenizer(text):\n",
    "    ''' \n",
    "    Maps a string to a list of byte tokens\n",
    "    '''\n",
    "    tokens = text.encode('utf-8')\n",
    "    tokens = list(map(int, tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to introduce the byte-pair encoding\n",
    "def get_pairs(bts):\n",
    "    \"\"\" \n",
    "    Returns a dictionary of all byte pairs in the input and their frequency\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    for pair in zip(bts, bts[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to merge the most frequent pair\n",
    "def merge_pairs(bts, pair, idx):\n",
    "    \"\"\" Merges the most frequent pair in the input\n",
    "    Args:\n",
    "        bts: list of byte tokens\n",
    "        pair: pair to merge\n",
    "        idx: new byte token to replace the pair\n",
    "    Returns:\n",
    "        new_bts: list of byte tokens with the most frequent pair merged\n",
    "    \"\"\"\n",
    "    new_bts = []\n",
    "    i = 0\n",
    "    while i < len(bts):\n",
    "        if i < len(bts) - 1 and bts[i] == pair[0] and bts[i + 1] == pair[1]:\n",
    "            new_bts.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_bts.append(bts[i])\n",
    "            i += 1\n",
    "    return new_bts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokens = byte_tokenizer(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[80, 101, 116, 105, 116]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##-------------------------------------------------##\n",
      "  Merging pair (101, 114) into new byte token 256\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (101, 32) into new byte token 257\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (32, 97) into new byte token 258\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (116, 105) into new byte token 259\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (100, 32) into new byte token 260\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (116, 104) into new byte token 261\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (32, 32) into new byte token 262\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (111, 110) into new byte token 263\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (105, 110) into new byte token 264\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (256, 32) into new byte token 265\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (101, 260) into new byte token 266\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (115, 32) into new byte token 267\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (116, 32) into new byte token 268\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (101, 110) into new byte token 269\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (46, 32) into new byte token 270\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (44, 32) into new byte token 271\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (116, 111) into new byte token 272\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (258, 110) into new byte token 273\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (259, 263) into new byte token 274\n",
      "##-------------------------------------------------##\n",
      "  Merging pair (111, 114) into new byte token 275\n"
     ]
    }
   ],
   "source": [
    "#First try of an implementation of the BPE algorithm for 20 merges\n",
    "#Hyperparameters\n",
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "bts = list(sample_tokens)\n",
    "\n",
    "#Main loop\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    pairs = get_pairs(bts)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best_pair = max(pairs, key=pairs.get)\n",
    "    btx = 256 + i\n",
    "    print('##-------------------------------------------------##')\n",
    "    print(f'  Merging pair {best_pair} into new byte token {btx}')\n",
    "    bts = merge_pairs(bts, best_pair, btx)\n",
    "    merges[best_pair] = btx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT-4 style BPE tokenizer\n",
    "class BasicTokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "        self.merges = {}\n",
    "\n",
    "    def get_pairs(self, bts):\n",
    "        counts = {}\n",
    "        for pair in zip(bts, bts[1:]):\n",
    "            counts[pair] = counts.get(pair, 0) + 1\n",
    "        return counts\n",
    "    \n",
    "    def merge_pairs(self, bts, pair, idx):\n",
    "        new_bts = []\n",
    "        i = 0\n",
    "        while i < len(bts):\n",
    "            if i < len(bts) - 1 and bts[i] == pair[0] and bts[i + 1] == pair[1]:\n",
    "                new_bts.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_bts.append(bts[i])\n",
    "                i += 1\n",
    "        return new_bts\n",
    "    \n",
    "            \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        ''' Train the tokenizer on the given text. '''\n",
    "        self.tokens = list(map(int, text.encode('utf-8')))\n",
    "        tokens = self.tokens\n",
    "        num_merges = vocab_size - 256\n",
    "        for i in range(num_merges):\n",
    "            pairs = self.get_pairs(tokens)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            btx = 256 + i\n",
    "            if verbose:\n",
    "                print(f'Merging pair {best_pair} into new byte token {btx}')\n",
    "            tokens = self.merge_pairs(tokens, best_pair, btx)\n",
    "            self.merges[best_pair] = btx\n",
    "            \n",
    "\n",
    "    def encode(self, text):\n",
    "        ''' Encode the text using the learned merges. '''\n",
    "        bts = list(map(int, text.encode('utf-8')))\n",
    "        for i in range(len(bts)):\n",
    "            for k, v in self.merges.items():\n",
    "                if i < len(bts) - 1 and bts[i] == k[0] and bts[i + 1] == k[1]:\n",
    "                    bts[i] = v\n",
    "                    bts.pop(i + 1)\n",
    "        return bts\n",
    "\n",
    "    def decode(self, ids):\n",
    "        ''' Decode the ids using the learned merges. '''\n",
    "        ids = list(ids)\n",
    "        for i in ids:\n",
    "            for k, v in self.merges.items():\n",
    "                if i == v:\n",
    "                    new_i = [k[0], k[1]]\n",
    "                    index = ids.index(i)\n",
    "                    ids.pop(index)\n",
    "                    ids[index:index] = new_i\n",
    "        text = ''.join(map(chr, ids))\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging pair (101, 114) into new byte token 256\n",
      "Merging pair (101, 32) into new byte token 257\n",
      "Merging pair (32, 97) into new byte token 258\n",
      "Merging pair (116, 105) into new byte token 259\n",
      "Merging pair (100, 32) into new byte token 260\n",
      "Merging pair (116, 104) into new byte token 261\n",
      "Merging pair (32, 32) into new byte token 262\n",
      "Merging pair (111, 110) into new byte token 263\n",
      "Merging pair (105, 110) into new byte token 264\n",
      "Merging pair (256, 32) into new byte token 265\n",
      "Merging pair (101, 260) into new byte token 266\n",
      "Merging pair (115, 32) into new byte token 267\n",
      "Merging pair (116, 32) into new byte token 268\n",
      "Merging pair (101, 110) into new byte token 269\n",
      "Merging pair (46, 32) into new byte token 270\n",
      "Merging pair (44, 32) into new byte token 271\n",
      "Merging pair (116, 111) into new byte token 272\n",
      "Merging pair (258, 110) into new byte token 273\n",
      "Merging pair (259, 263) into new byte token 274\n",
      "Merging pair (111, 114) into new byte token 275\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BasicTokenizer()\n",
    "vocab_size = 276\n",
    "tokenizer.train(sample_text, vocab_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = '''Copy paste of the Wikipedia article on Taylor Swift, as of Feb 16, 2024.\n",
    "---\n",
    "\n",
    "Main menu\n",
    "\n",
    "WikipediaThe Free Encyclopedia\n",
    "\n",
    "Search\n",
    "Create account\n",
    "Log in\n",
    "\n",
    "Personal tools\n",
    "Contents  hide\n",
    "(Top)\n",
    "Life and career\n",
    "Toggle Life and career subsection\n",
    "Artistry\n",
    "Toggle Artistry subsection'''\n",
    "encoded = tokenizer.encode(new_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Copy paste of the Wikipedia article on Taylor Swift, as of Feb 16, 2024.\\n---\\n\\nMain menu\\n\\nWikipediaThe Free Encyclopedia\\n\\nSearch\\nCreate account\\nLog in\\n\\nPersonal tools\\nContents  hide\\n(Top)\\nLife and career\\nToggle Life and careĀ subsection\\nArtistry\\nToggle Artistry subsection'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = tokenizer.decode(encoded)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexTokenizer(BasicTokenizer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #This is the GPT-4 regex pattern\n",
    "        self.pattern = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        ''' Train the tokenizer on the given text. '''\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokenizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
